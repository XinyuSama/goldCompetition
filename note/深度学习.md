深度学习和机器学习的区别？深度学习解决的是怎么用去提取特征 比 机器学习还要智能

主要用在

- 计算机视觉-cv+卷积神经网络
- 自然语言处理-递归神经网络
- 医学

移动端支持不太好 深度学习计算量大

深度学习是机器学习的核心

![img.png](F:\Project\43.goldCompetition\note\note.assets\img.png)

数据规模越大越好

# 1.计算机视觉任务
## 1.1 图像分类任务

一张图片被表示成三维数据的形式，每个像素的值从0到255
![img_2.png](F:\Project\43.goldCompetition\note\note.assets\img_2.png)

数值越大的表示该点越亮

## 1.2 计算机视觉面临的挑战
![img_1.png](F:\Project\43.goldCompetition\note\note.assets\img_1.png)
![img_3.png](F:\Project\43.goldCompetition\note\note.assets\img_3.png)

## 1.3 机器学习常规套路
![img_4.png](F:\Project\43.goldCompetition\note\note.assets\img_4.png)

## 1.4 k近邻（了解）
![img_5.png](F:\Project\43.goldCompetition\note\note.assets\img_5.png)
![img_6.png](F:\Project\43.goldCompetition\note\note.assets\img_6.png)
![img_7.png](F:\Project\43.goldCompetition\note\note.assets\img_7.png)

### 1.4.1 数据库样例

![img_8.png](F:\Project\43.goldCompetition\note\note.assets\img_8.png)

### 1.4.2 图像分类
#### 1.4.2.1 计算距离
456是所有单位数值计算后相加的结果
![img_9.png](F:\Project\43.goldCompetition\note\note.assets\img_9.png)
遍历计算所有图片 把距离相等相当的 图像罗列出来
![img_10.png](F:\Project\43.goldCompetition\note\note.assets\img_10.png)

#### 1.4.2.2 问题出在哪？
* 没有告诉哪块是主体
* 哪块是背景
* k近邻不知道哪块是主题哪块是主体，所以不适合做深度学习图像处理
![img_11.png](F:\Project\43.goldCompetition\note\note.assets\img_11.png)
# 2.神经网络基础
## 2.1 线性函数（得分函数）

![img_12.png](F:\Project\43.goldCompetition\note\note.assets\img_12.png)
10分类问题
![img_13.png](F:\Project\43.goldCompetition\note\note.assets\img_13.png)
每一个像素点对应一个权重参数（W)
b起微调参数

## 2.2 计算方法
![img_14.png](F:\Project\43.goldCompetition\note\note.assets\img_14.png)
<font color =red>什么样的W最适合 神经网络整个周期就是一直再找合适的W权重</font>
![img_15.png](F:\Project\43.goldCompetition\note\note.assets\img_15.png)

## 2.3 损失函数
结果如何评判呢，就需要一个损失值(loss)
越低越好，衡量这个权重(w)的结果如何
![img_16.png](F:\Project\43.goldCompetition\note\note.assets\img_16.png)

### 2.3.1 有损失函数后的改进

## ![image-20221002014139981](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002014139981.png)

加入正则化R(W)惩罚项

## ![image-20221002014240944](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002014240944.png)

λ即惩罚系数，太大则不希望过拟合，不希望变异，小就意思意思，强调模型太过复杂，神经网络太过强大任意过拟合。

## 2.4 Softmax分类器

![image-20221002014619825](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002014619825.png)



## 2.5 前向传播

![image-20221002014719035](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002014719035.png)

回归任务：由得分值去计算一个损失

分类任务：由概率值去计算一个损失

神经网络主要分为两大块：前向传播和反向传播

前向传播：有x和w怎么样得出来一个loss损失

> 《写给小白的神经网络前向传播原理详解》
>
> https://blog.csdn.net/zbp_12138/article/details/108272563

## 2.6 反向传播(梯度下降)

反向传播->梯度下降去更新w再更新模型

![image-20221002015327281](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002015327281.png)

逐层从前往后传播

![image-20221002015620658](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002015620658.png)

> 《写给小白的入门项目，神经网络前向传播原理详解》
>
> https://aistudio.baidu.com/aistudio/projectdetail/742777

## 2.7 神经网络整体架构

层次结构，一层一层的来变换数据的。

![image-20221001182344505](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001182344505.png)

`input layer`即输入的特征数 

神经元越多越好，但是容易过拟合。神经元增加一个，增加一组。

## 2.8 正则化和激活函数，池化层

为什么需要正则化？可以减少过拟合
正则化方式：L1正则化，L2正则化（使用更多）
Batch Norm： 归一化，加速学习

### 2.8.1 非线性激活函数示例（z为自变量）

***激活函数 sigmoid(z)\***：取值范围为（0,1），适用于二元分类（0,1）的输出层
 ***激活函数 tanh(z)\***：取值范围为（-1,1），适用于中间的隐藏层，因为有类似于数据中心化的作用，在大多数情况下tanh比sigmoid要好一些。
 sigmoid函数和tanh函数在z非常大或者非常小的时候，斜率接近为0，会拖慢算法的学习效率，解决办法：Relu函数
 ***激活函数 ReLu(z)\***：取值范围为0和1，解决了sigmoid和tanh在z取值较大时
 ***激活函数 Leaky ReLu(z)\***：进阶版的ReLu，一般来说效果更好，但使用频率没有ReLu函数多
 使用在ReLu函数，理论上有一半的z的ReLu的导数为0，但在实践中，有足够多的为正数的z，所以对大多数训练样本来说还是很快的。使用ReLu神经网络的学习速度会比sigmoid和tanh快很多

### 2.8.2 池化层

作用：压缩数据

# 3.搭建神经网络进行回归任务

## 3.1 基于Keras构建网络模型

- activation:激活函数的选择，一般常用relu
- kernel_initalizer,bias_initializer:权重与偏置参数的初始化方法，有时候不收敛种初始化就突然不好使了。。。玄学
- kernel_regularizer,bias_regularizer:要不要加入正则化
- inputs:输入，可以自己指定，也可以让网络自动选
- units:神经元个数

### 3.1.1 按顺序构造网络模型

```python
model = tf.keras.Sequential()
# 创建三个全连接隐层 第一个为16个神经元 ，第二个为32个，最后一个为1即输出最后一个
model.add(layers.Dense(16))
model.add(layers.Dense(32))
model.add(layers.Dense(1)) # 做回归任务 最后返回就一个值 一般就分回归和分类问题
# Dense即全连接 wx+b
# 更多 api 官网 https://tensorflow.google.cn/api_docs/python/tf
# 卷积 Cropping
```

![image-20221001181936846](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001181936846.png)

### 3.1.2 对网络进行配置

```python
# compile相当于对网络进行配置，指定好优化器和损失函数等
model.compile(optimizer=tf.keras.optimizers.SGD(0.001),loss='mean_squared_error')
```

### 3.1.3 进行训练

```python
# 训练
# 所有的x ; y ; 0.25的验证集 ; 整个数据集遍历10遍 ; 每一次迭代64个样本
# 是当然也可以 手动划分训练集和测试集 分别传入
model.fit(input_features,labels,validation_split=0.25,epochs=10,batch_size=64)
```

### 3.1.4 展示网络结构
`model.summary()`

### 3.1.5 调节每一层配置

#### 3.1.5.1 更改初始化方法

使用`随机高斯分布`来设置`权重参数`

`kernel_initializer='random_normal'`

```python
model = tf.keras.Sequential()
model.add(layers.Dense(16,kernel_initializer='random_normal'))
model.add(layers.Dense(32,kernel_initializer='random_normal'))
model.add(layers.Dense(1,kernel_initializer='random_normal'))
```

加入`正则化惩罚项`

`kernel_regularizer=tf.keras.regularizers.l2(0.03)`

```python
model = tf.keras.Sequential()
model.add(layers.Dense(16,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))
model.add(layers.Dense(32,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))
model.add(layers.Dense(1,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))
```

验证集更有说服力。

3.1.6 气温预测-回归问题整代码

```python
"""
 * description: 搭建神经网络进行气温预测
 * date: 2022/10/01/15:56:00
 * author: xin yu
 * version: 1.0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow.keras
import warnings
import datetime
from sklearn import preprocessing
warnings.filterwarnings('ignore')

features = pd.read_csv('temps.csv')

# 数据样子
print(features)

# 处理时间数据

# 分别得到年,月,日

years = features['year']
months = features['month']
days = features['day']

# datetime格式
dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year,month,day in zip(years,months,days)]
dates = [datetime.datetime.strptime(date,'%Y-%m-%d') for date in dates]

# 准备画图
# 指定默认风格
# plt.style.use('fivethirtypeight')

# 设置布局
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2,ncols=2,figsize = (10,10)) # 创建四个画布 类似解构赋值

print(ax1,ax2)

fig.autofmt_xdate(rotation=45)
#
# 标签值
ax1.plot(dates,features['actual'])
ax1.set_xlabel(''); ax1.set_ylabel('Temperature'); ax1.set_title('Max Temp')

# 昨天
ax2.plot(dates,features['temp_1'])
ax2.set_xlabel(''); ax2.set_ylabel('Temperature'); ax2.set_title('Previous Max Temp')

# 前天
ax3.plot(dates,features['temp_2'])
ax3.set_xlabel('Date'); ax3.set_ylabel("Temperature"); ax3.set_title("Two Days Prior Max Temp")

# 我的逗逼朋友
ax4.plot(dates,features['friend'])
ax4.set_xlabel('Date'); ax4.set_ylabel("Temperature"); ax4.set_title("Friend Estimate")

plt.tight_layout(pad=2)
plt.show()

# 独热编码  处理week原来为字符串 改为数值
features = pd.get_dummies(features)
features.head(5)
print(features)

# 标签 拿到y
labels = np.array(features['actual'])

# 在特征中去掉标签 拿到x
features = features.drop('actual',axis=1)

# 名字单独保存一下，以备后患
features_list = list(features.columns)

# 转换成合适的格式
features = np.array(features)
print(features)

# 使用sklearn 做数据预处理 数据标准化
input_features = preprocessing.StandardScaler().fit_transform(features)

print(input_features)

# 按顺序构造网络模型
model = tf.keras.Sequential()
model.add(layers.Dense(16,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))
model.add(layers.Dense(32,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))
model.add(layers.Dense(1,kernel_initializer='random_normal',kernel_regularizer=tf.keras.regularizers.l2(0.03)))

# compile相当于对网络进行配置，指定好优化器和损失函数等
model.compile(optimizer=tf.keras.optimizers.SGD(0.001),loss='mean_squared_error')
# 训练
# 所有的x ; y ; 0.25的验证集 ; 整个数据集遍历10遍 ; 每一次迭代64个样本
# 是当然也可以 手动划分训练集和测试集 分别传入
model.fit(input_features,labels,validation_split=0.25,epochs=100,batch_size=64)

# 展示网络结构
model.summary()

# 预测模型结果
predict = model.predict(input_features)
print(predict)

# 测试结果并进行展示
dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year,month,day in zip(years,months,days)]
dates = [datetime.datetime.strptime(date,'%Y-%m-%d') for date in dates]

# 创建一个表格来存日期和其对应的标签数值
true_data = pd.DataFrame(data={'date':dates,'actual':labels})

# 同理，再创键一个来存日期和其对应的模型预测值
months = features[:,features_list.index('month')]
days = features[:,features_list.index('day')]
years = features[:,features_list.index('year')]

test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year,month,day in zip(years,months,days)]

test_dates = [datetime.datetime.strptime(date,'%Y-%m-%d') for date in test_dates]

predictions_data = pd.DataFrame(data={'date':test_dates,'prediction':predict.reshape(-1)})

# 真实值
plt.plot(true_data['date'],true_data['actual'],'b-',label = 'actual')

# 预测值
plt.plot(predictions_data['date'],predictions_data['prediction'],'ro',label = 'prediction')
plt.xticks(rotation =60) # x轴的子倾斜角
plt.legend()
# 图名
plt.xlabel('Date')
plt.ylabel("Maximum Temperature(F)")
plt.title('Actual and Predicted Values')
plt.show()
```

# 4.搭建神经网络进行分类任务

![image-20221001204447912](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001204447912.png)

其实无论是回归还是分类，其实都是把数据传入模型里提取特征，最后把特征传入Softmax里

最后一个隐藏改为，10，初始化改为Softmax，即输出10个结果，0-9每个的概率值，回归问题我们只需要得到1个结果值，但是分类不一样

 `一定选择对应适合的损失函数,不同的损失函数需要的数据格式不一样,否则会报错`

 `比如 `one_hot`  。。。值等等等等

> 什么是one hot编码？为什么要使用one hot编码？
>
> 你可能在有关机器学习的很多文档、文章、论文中接触到“one hot编码”这一术语。本文将科普这一概念，介绍one hot编码到底是什么。
>
> 一句话概括：**one hot编码是将类别变量转换为机器学习算法易于利用的一种形式的过程。**
>
> 通过例子可能更容易理解这个概念。
>
> 假设我们有一个迷你数据集：
>
> ![img](https://pic1.zhimg.com/80/v2-5169c7bde2fa839aca7377a5080a5ca0_720w.webp)
>
> 其中，类别值是分配给数据集中条目的数值编号。比如，如果我们在数据集中新加入一个公司，那么我们会给这家公司一个新类别值4。当独特的条目增加时，类别值将成比例增加。
>
> 在上面的表格中，类别值从1开始，更符合日常生活中的习惯。实际项目中，类别值从0开始（因为大多数计算机系统计数），所以，如果有N个类别，类别值为0至N-1.
>
> sklear的LabelEncoder可以帮我们完成这一类别值分配工作。
>
> 现在让我们继续讨论one hot编码，将以上数据集one hot编码后，我们得到的表示如下：
>
> ![img](https://pic1.zhimg.com/80/v2-e5e6c2b72a4a07b8b7e056ba68667488_720w.webp)
>
> 在我们继续之前，你可以想一下为什么不直接提供标签编码给模型训练就够了？为什么需要one hot编码？
>
> 标签编码的问题是它假定类别值越高，该类别更好。“等等，什么！”
>
> 让我解释一下：根据标签编码的类别值，我们的迷你数据集中VW > Acura > Honda。比方说，假设模型内部计算平均值（神经网络中有大量加权平均运算），那么1 + 3 = 4，4 / 2 = 2. 这意味着：VW和Honda平均一下是Acura。毫无疑问，这是一个糟糕的方案。该模型的预测会有大量误差。
>
> 我们使用one hot编码器对类别进行“二进制化”操作，然后将其作为模型训练的特征，原因正在于此。
>
> 当然，如果我们在设计网络的时候考虑到这点，对标签编码的类别值进行特别处理，那就没问题。不过，在大多数情况下，使用one hot编码是一个更简单直接的方案。
>
> 另外，如果原本的标签编码是有序的，那one hot编码就不合适了——会丢失顺序信息。
>
> 最后，我们用一个例子总结下本文：
>
> 假设“花”的特征可能的取值为`daffodil`（水仙）、`lily`（百合）、`rose`（玫瑰）。one hot编码将其转换为三个特征：`is_daffodil`、`is_lily`、`is_rose`，这些特征都是二进制的。



```python
"""
 * description: 分类任务
 * date: 2022/10/01/20:59:00
 * author: xinyu
 * version: 1.0
"""

from pathlib import Path
import requests
import pickle
import gzip
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from sklearn import preprocessing
import tensorflow.keras
# 下载mnist数据集
DATA_PATH = Path('data')
PATH = DATA_PATH/"mnist"
#
PATH.mkdir(parents=True,exist_ok=True)
#
# URL = "http://deeplearning.net/data/mnist/"
FILENAME = "mnist.pkl.gz"
#
# if not (PATH/FILENAME).exists():
#     content = requests.get(URL+FILENAME).content
#     (PATH/FILENAME).open('wb').write(content)

# with gzip.open((PATH/FILENAME).as_posix(),'rb') as f :
#     ((x_train,y_train),(x_valid,y_valid),_) = pickle.load(f,encoding='latin-1')

# 下载访问不了 手动下载 解包
with open(PATH/'mnist.pkl', 'rb') as f:
    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')


# 一定选择对应适合的损失函数
model = tf.keras.Sequential()
model.add(layers.Dense(16,activation='relu'))
model.add(layers.Dense(32,activation='relu'))
model.add(layers.Dense(10,activation='softmax'))

# compile相当于对网络进行配置，指定好优化器和损失函数等
model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])

model.fit(x_train,y_train,epochs=5,batch_size=64,validation_data=(x_valid,y_valid))
```

## 4.1 tf-data模块

用来方便我们构建训练数据格式

### 4.1.2 ndarray转tensor格式张量

```python
input_data = np.arange(16) # array([0,1,2......])
dataset = tf.data.Dataset.from_tensor_slices(input_data)
for data in dataset:
    print(data)
```

![image-20221001234257162](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001234257162.png)

### 4.1.3 repeat操作

把数据重复几份，返回为几倍的tensor数据

```python
dataset = tf.data.Dataset.from_tensor_slices(input_data)
dataset = dataset.repeat(2)
for data dataset:
    print(data)
```

![image-20221001234632969](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001234632969.png)

### 4.1.4 batch操作

分组

![image-20221001234734211](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001234734211.png)

### 4.1.5 shuffle操作

洗牌，弥补batch的有规则分组

![image-20221001234849543](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221001234849543.png)

## 4.2 练手fashion数据集

```python
"""
 * description: 图像分类
 * date: 2022/10/01/23:52:00
 * author: xinyu
 * version: 1.0
"""
from tensorflow import keras
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from sklearn import preprocessing

# import tensorflow.keras

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

print(test_images.shape)
print(len(train_labels))

# 初步查看数据

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

# 预处理
train_images = train_images / 255.0
test_images = test_images / 255.0
# 展示前25张图
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()

# 第一个层 把我的数据进行拉长操作 把图像的三维 拉成一个 28*28的向量
model = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),
                          keras.layers.Dense(128, activation='relu'),
                          keras.layers.Dense(10, activation='softmax')
                          ])

model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])

model.fit(train_images, train_labels, epochs=10)

# 评估操作

test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)

print('\nTest accuracy:', test_acc)

# 预测
predictions = model.predict(test_images)
print(predictions.shape)
# 取第一个样本 对应十个类别的概率值 ， 一般取最大的一个
print(predictions[0])
print(np.argmax(predictions[0]))


# 结果进行可视化

def plot_image(i, predictions_array, true_label, img):
    predictions_array, true_label, img = predictions_array, true_label[i], img[i]
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])

    plt.imshow(img, cmap=plt.cm.binary)

    predicted_label = np.argmax(predictions_array)
    if predicted_label == true_label:
        color = 'blue'
    else:
        color = 'red'

    plt.xlabel('{} {:2.0f}% ({})'.format(class_names[predicted_label], 100 * np.max(predictions_array),
                                         class_names[true_label]), color=color)


def plot_value_array(i, predictions_array, true_label):
    predictions_array, true_label = predictions_array, true_label[i]
    plt.grid(False)
    plt.xticks(range(10))
    plt.yticks([])
    thisplot = plt.bar(range(10), predictions_array, color='#777777')
    predicted_label = np.argmax(predictions_array)

    thisplot[predicted_label].set_color('red')
    thisplot[true_label].set_color('blue')


i = 0
plt.figure(figsize=(6, 3))
plt.subplot(1, 2, 1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1, 2, 2)
plot_value_array(i, predictions[i], test_labels)
plt.show()

i = 12
plt.figure(figsize=(6, 3))
plt.subplot(1, 2, 1)
plot_image(i, predictions[i], test_labels, test_images)
plt.subplot(1, 2, 2)
plot_value_array(i, predictions[i], test_labels)
plt.show()

# 保存训练好的模型
# 保存权重参数和网络模型

model.save('fashion_model.h5')
model.save_weights('weights.h5')
# 网络架构
config = model.to_json()
print(config)
```

### 4.2.1 模型保存与读取实列进行测试

```python
# 保存训练好的模型
# 保存权重参数和网络模型

model.save('fashion_model.h5')
model.save_weights('weights.h5')
# 网络架构
config = model.to_json()
print(config)
```

读取模型测试

```python
"""
 * description: fashion模型导入进行测试
 * date: 2022/10/02/00:57:00
 * author: xin yu
 * version: 1.0
"""

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow import keras
import matplotlib.pyplot as plt
import numpy as np
import fashion as fa

model = keras.models.load_model('fashion_model.h5')

fashion_mnist = keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# 测试的时候需要对测试数据进行和训练的时候相同的预处理
# 预处理
train_images = train_images / 255.0
test_images = test_images / 255.0

# 预测结果
predictions = model.predict(test_images)
print(predictions.shape)

num_rows = 5
num_cols = 3
num_images = num_cols * num_rows
plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
for i in range(num_images):
    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)
    fa.plot_image(i, predictions[i], test_labels, test_images)
    plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)
    fa.plot_value_array(i, predictions[i], test_labels)
plt.tight_layout()
plt.show()
```



# 5.卷积神经网络(CNN)原理与参数解读

## 5.1 卷积网络和传统网络的区别

![image-20221002012754166](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002012754166.png)

2维一列特征值矩阵和3维数据矩阵的区别

值 vs H x W x C

### 5.1.1 整体架构：

![image-20221002013014828](F:\Project\43.goldCompetition\note\深度学习.assets\image-20221002013014828.png)
